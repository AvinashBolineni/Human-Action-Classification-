{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activity Recognition using RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gq3BsxawcYyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. "
      ]
    },
    {
      "metadata": {
        "id": "Rkj0lZMhcY0_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Notation**:\n",
        "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
        "    - Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
        "\n",
        "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
        "    - Example: $x^{(i)}$ is the $i^{th}$ training example input.\n",
        "\n",
        "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
        "    - Example: $x^{\\langle t \\rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\\langle t \\rangle}$ is the input at the $t^{th}$ timestep of example $i$.\n",
        "    \n",
        "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
        "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rG1EltVycY8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
        " \n",
        "The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. "
      ]
    },
    {
      "metadata": {
        "id": "yotqgB3IcY3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://imgur.com/Yaa79IN.png\" style=\"width:500;height:300px;\">\n",
        "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
      ]
    },
    {
      "metadata": {
        "id": "ncAhbvAxcY5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's how you can implement an RNN: \n",
        "\n",
        "**Code Instructions**:\n",
        "1. Implement the calculations needed for one time-step of the RNN.\n",
        "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. \n",
        "\n",
        "Let's go!\n",
        "\n",
        "## 1.1 - RNN cell\n",
        "\n",
        "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
        "\n",
        "<img src=\"https://imgur.com/vGxAY57.png\" style=\"width:700px;height:300px;\">\n",
        "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
        "\n",
        "\n",
        "**Code Instructions**:\n",
        "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
        "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
        "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
        "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
        "\n",
        "We will vectorize over $m$ examples. Thus, $x^{\\langle t \\rangle}$ will have dimension $(n_x,m)$, and $a^{\\langle t \\rangle}$ will have dimension $(n_a,m)$. "
      ]
    },
    {
      "metadata": {
        "id": "e1YomYKhdyvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 - RNN forward pass \n",
        "\n",
        "RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell tak\n",
        "es as input the hidden state from the previous cell ($a^{\\langle t-1 \\rangle}$) and the current time-step's input data ($x^{\\langle t \\rangle}$). It outputs a hidden state ($a^{\\langle t \\rangle}$) and a prediction ($y^{\\langle t \\rangle}$) for this time-step.\n",
        "\n",
        "\n",
        "<img src=\"https://imgur.com/YdNCgkN.png\" style=\"width:800px;height:300px;\">\n",
        "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
        "\n",
        "**Code Instructions**:\n",
        "1. Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN.\n",
        "2. Initialize the \"next\" hidden state as $a_0$ (initial hidden state).\n",
        "3. Start looping over each time step, your incremental index is $t$ :\n",
        "    - Update the \"next\" hidden state and the cache by running `rnn_cell_forward`\n",
        "    - Store the \"next\" hidden state in $a$ ($t^{th}$ position) \n",
        "    - Store the prediction in y\n",
        "    - Add the cache to the list of caches\n",
        "4. Return $a$, $y$ and caches\n"
      ]
    },
    {
      "metadata": {
        "id": "pC17Ysu7dyzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 - Basic RNN  backward pass\n",
        "\n",
        "We will start by computing the backward pass for the basic RNN-cell.\n",
        "\n",
        "<img src=\"https://imgur.com/3EniMu4.png\" style=\"width:500;height:300px;\"> <br>\n",
        "<caption><center> **Figure 4**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
      ]
    },
    {
      "metadata": {
        "id": "L6pw-MLudy3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Deriving the one step backward functions: \n",
        "\n",
        "To compute the `rnn_cell_backward` you need to compute the following equations. It is a good exercise to derive them by hand. \n",
        "\n",
        "The derivative of $\\tanh$ is $1-\\tanh(x)^2$.\n",
        "\n",
        "Similarly for $\\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{ax}}, \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{aa}},  \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial b}$, the derivative of  $\\tanh(u)$ is $(1-\\tanh(u)^2)du$. \n",
        "\n",
        "The final two equations also follow same rule and are derived using the $\\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match."
      ]
    },
    {
      "metadata": {
        "id": "P5NjmBkUdy6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Backward pass through the RNN\n",
        "\n",
        "Computing the gradients of the cost with respect to $a^{\\langle t \\rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Implement the `rnn_backward` function. Initialize the return variables with zeros first and then loop through all the time steps while calling the `rnn_cell_backward` at each time timestep, update the other variables accordingly."
      ]
    },
    {
      "metadata": {
        "id": "KprcSzeady9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. "
      ]
    },
    {
      "metadata": {
        "id": "GwUKMxiEdy2A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Long Short-Term Memory (LSTM) network\n",
        "\n",
        "This following figure shows the operations of an LSTM-cell.\n",
        "\n",
        "<img src=\"https://imgur.com/wRyYVQ6.png\" style=\"width:500;height:400px;\">\n",
        "<caption><center> **Figure 4**: LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>\n",
        "\n",
        "Similar to the RNN example above, you will start by understanding the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. \n",
        "\n",
        "### About the gates\n",
        "\n",
        "#### - Forget gate\n",
        "\n",
        "For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: \n",
        "\n",
        "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
        "\n",
        "Here, $W_f$ are weights that govern the forget gate's behavior. We concatenate $[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$ and multiply by $W_f$. The equation above results in a vector $\\Gamma_f^{\\langle t \\rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\\langle t-1 \\rangle}$. So if one of the values of $\\Gamma_f^{\\langle t \\rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\\langle t-1 \\rangle}$. If one of the values is 1, then it will keep the information. \n",
        "\n",
        "#### - Update gate\n",
        "\n",
        "Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: \n",
        "\n",
        "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
        "\n",
        "Similar to the forget gate, here $\\Gamma_u^{\\langle t \\rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\\tilde{c}^{\\langle t \\rangle}$, in order to compute $c^{\\langle t \\rangle}$.\n",
        "\n",
        "#### - Updating the cell \n",
        "\n",
        "To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: \n",
        "\n",
        "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
        "\n",
        "Finally, the new cell state is: \n",
        "\n",
        "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
        "\n",
        "\n",
        "#### - Output gate\n",
        "\n",
        "To decide which outputs we will use, we will use the following two formulas: \n",
        "\n",
        "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
        "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$\n",
        "\n",
        "Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\\tanh$ of the previous state. "
      ]
    },
    {
      "metadata": {
        "id": "RbBIkGOleVZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 - LSTM cell\n",
        "\n",
        "**Instructions**:\n",
        "1. Concatenate $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$ in a single matrix: $concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$\n",
        "2. Compute all the formulas 1-6. You can use `sigmoid()` and `np.tanh()`.\n",
        "3. Compute the prediction $y^{\\langle t \\rangle}$. You can use `softmax()`"
      ]
    },
    {
      "metadata": {
        "id": "W5lLIj0peYlJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Forward pass for LSTM\n",
        "\n",
        "Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. \n",
        "\n",
        "<img src=\"https://imgur.com/CFEgAAx.png\" style=\"width:500;height:300px;\">\n",
        "<caption><center> **Figure 5**: LSTM over multiple time-steps. </center></caption>\n",
        "\n",
        "**Exercise:** Implement `lstm_forward()` to run an LSTM over $T_x$ time-steps. \n",
        "\n",
        "**Note**: $c^{\\langle 0 \\rangle}$ is initialized with zeros."
      ]
    },
    {
      "metadata": {
        "id": "xyvTSphqeYWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. Now we will see how to do backpropagation in LSTM  and RNNS\n"
      ]
    },
    {
      "metadata": {
        "id": "4VE20Fh9eYdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3- LSTM backward pass"
      ]
    },
    {
      "metadata": {
        "id": "sg_ZpZwEejTh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 One Step backward\n",
        "\n",
        "The LSTM backward pass is slighltly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) \n",
        "\n",
        "### 2.3.2 gate derivatives\n",
        "\n",
        "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
        "\n",
        "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
        "\n",
        "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
        "\n",
        "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
        "\n",
        "### 2.3.3 parameter derivatives \n",
        "\n",
        "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
        "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
        "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
        "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
        "\n",
        "To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$ respectively. Note that you should have the `keep_dims = True` option.\n",
        "\n",
        "Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.\n",
        "\n",
        "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
        "Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...)\n",
        "\n",
        "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
        "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
        "where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...)\n"
      ]
    },
    {
      "metadata": {
        "id": "48IPf1r-ejXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Backward pass through the LSTM RNN\n",
        "\n",
        "This part is very similar to the `rnn_backward` function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. \n",
        "\n",
        "**Instructions**: Implement the `lstm_backward` function. Create a for loop starting from $T_x$ and going backward. For each step call `lstm_cell_backward` and update the your old gradients by adding the new gradients to them. Note that `dxt` is not updated but is stored."
      ]
    },
    {
      "metadata": {
        "id": "x993luPGejcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Questions and Answers(Q&A)"
      ]
    },
    {
      "metadata": {
        "id": "xtS4D1qTejk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In LSTM Network (Understanding LSTMs), Why input gate and output gate use tanh? what is the intuition behind this?\n",
        "\n",
        "A: The reason for using tanh is that its range is between (-1,1) whereas the sigmoid function is (0,1).Actually tanh is extended version of sigmoid.\n",
        "$$ tanh = 2* \\sigma(2x) - 1 $$\n",
        "So, the gradient of the tanh is almost twice than the sigmoid gradient in the x range (-1.663,1.663) and is almost equal to sigmoid gradient in the x range (-inf,-1.633) U (1.663,inf). So the convergence is faster in the case of tanh because of larger gradients and is more resistant to the vanishing gradient problems."
      ]
    },
    {
      "metadata": {
        "id": "IvXjAHsbcKY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Activity Recognition"
      ]
    },
    {
      "metadata": {
        "id": "zOHQ7a5_aqUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "from keras.utils.io_utils import HDF5Matrix\n",
        "\n",
        "SEQ_LEN = 30\n",
        "MAX_SEQ_LEN = 200\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgxT_-jFB-kG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "db8123f0-ec66-4f40-c8e8-e2ab74353e7a"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mohanrajmit/Human-Action-Classification-.git "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Human-Action-Classification-'...\n",
            "remote: Enumerating objects: 1260, done.\u001b[K\n",
            "remote: Counting objects: 100% (1260/1260), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1255/1255), done.\u001b[K\n",
            "remote: Total 1260 (delta 15), reused 1236 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1260/1260), 469.56 MiB | 33.00 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Checking out files: 100% (1218/1218), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "doq8RcCGCFNx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rQW1WYscPY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get model with pretrained weights.\n",
        "    base_model = InceptionV3(\n",
        "    weights='imagenet',\n",
        "    include_top=True)\n",
        "    \n",
        "    \n",
        "    # We'll extract features at the final pool layer.\n",
        "    model = Model(\n",
        "        inputs=base_model.input,\n",
        "        outputs=base_model.get_layer('avg_pool').output)\n",
        "    \n",
        "    # Getting the data\n",
        "    df = get_data('Human-Action-Classification-/data/data_file.csv')\n",
        "    \n",
        "    # Clean the data\n",
        "    df_clean = clean_data(df)\n",
        "    \n",
        "    # Creating index-label maps and inverse_maps\n",
        "    label_index, index_label = get_class_dict(df_clean)\n",
        "    \n",
        "    # Split the dataset into train and test\n",
        "    train, test = split_train_test(df_clean)\n",
        "    \n",
        "    # Encoding the dataset\n",
        "    encode_dataset(train, model, label_index, \"train\")\n",
        "    encode_dataset(test,model,label_index,\"test\")\n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PhOa06wVIUZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "08a52420-d285-4446-9ba1-5be3fce96c6b"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6bbbd83ef674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDF5Matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_8.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9df399d0acb2>\u001b[0m in \u001b[0;36mlstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m#model.add(Dense(10, activation='softmax'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_index' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6CSH8rX1IKYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0fc374d1-60da-442a-bb8f-c8f5dff2b832"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human-Action-Classification-  sample_data  test_8.h5  train_8.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M9-tCCZscPW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(path, if_pd=False):\n",
        "    \"\"\"Load our data from file.\"\"\"\n",
        "    names = ['partition', 'class', 'video_name', 'frames']\n",
        "    df = pd.read_csv(path,names=names)\n",
        "    return df\n",
        "\n",
        "def get_class_dict(df):\n",
        "    class_name =  list(df['class'].unique())\n",
        "    index = np.arange(0, len(class_name))\n",
        "    label_index = dict(zip(class_name, index))\n",
        "    index_label = dict(zip(index, class_name))\n",
        "    return (label_index, index_label)\n",
        "    \n",
        "def clean_data(df):\n",
        "    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
        "    df = df[mask]\n",
        "    return df\n",
        "def split_train_test(df):\n",
        "    partition =  (df.groupby(['partition']))\n",
        "    un = df['partition'].unique()\n",
        "    train = partition.get_group(un[0])\n",
        "    test = partition.get_group(un[1])\n",
        "    return (train, test)\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (227,227))\n",
        "    return preprocess_input(img)\n",
        "    \n",
        "    \n",
        "def encode_video(row, model, label_index):\n",
        "    cap = cv2.VideoCapture(os.path.join(\"Human-Action-Classification-/data\",\"UCF-101\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".avi\"))\n",
        "    images = []  \n",
        "    for i in range(SEQ_LEN):\n",
        "        ret, frame = cap.read()\n",
        "        frame = preprocess_image(frame)\n",
        "        images.append(frame)\n",
        "    \n",
        "    \n",
        "    features = model.predict(np.array(images))\n",
        "    index = label_index[row[\"class\"].iloc[0]]\n",
        "    y_onehot = to_categorical(index, len(label_index.keys()))\n",
        "    \n",
        "    return features, y_onehot\n",
        "\n",
        "def encode_dataset(data, model, label_index, phase):\n",
        "    input_f = []\n",
        "    output_y = []\n",
        "    required_classes = [\"ApplyEyeMakeup\" , \"ApplyLipstick\" , \"Archery\" , \"BabyCrawling\" , \"BalanceBeam\" ,\n",
        "                       \"BandMarching\" , \"BaseballPitch\" , \"Basketball\" , \"BasketballDunk\"]\n",
        "   \n",
        "    \n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "    # Check whether the given row , is of a class that is required\n",
        "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
        " \n",
        "            features,y =  encode_video(data.iloc[[i]], model, label_index)\n",
        "            input_f.append(features)\n",
        "            output_y.append(y)\n",
        "        \n",
        "    \n",
        "    f = h5py.File(phase+'_8'+'.h5', 'w')\n",
        "    f.create_dataset(phase, data=np.array(input_f))\n",
        "    f.create_dataset(phase+\"_labels\", data=np.array(output_y))\n",
        "    \n",
        "    del input_f[:]\n",
        "    del output_y[:]\n",
        "\n",
        "def lstm():\n",
        "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "    our CNN to this model predominantly.\"\"\"\n",
        "    input_shape = (SEQ_LEN, 2048)\n",
        "    # Model.\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(2048, return_sequences=False,\n",
        "                   input_shape=input_shape,\n",
        "                   dropout=0.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(label_index.keys()), activation='softmax'))\n",
        "    #model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(filepath='data/models/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
        "    \n",
        "    tb_callback = TensorBoard(log_dir=\"logs\",histogram_freq=2,write_graph=True)\n",
        "    \n",
        "    early_stopping = EarlyStopping(monitor = 'val_loss',patience= 10)\n",
        "    \n",
        "    callback_list = [checkpoint, tb_callback]\n",
        "\n",
        "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=metrics)\n",
        "    return model, callback_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnkJp3ljcPam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "c8b5a4d0-444d-40c6-cbb5-2ae56fd91ee9"
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 703/703 [02:45<00:00,  4.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-bb1f88819164>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Encoding the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9df399d0acb2>\u001b[0m in \u001b[0;36mencode_dataset\u001b[0;34m(data, model, label_index, phase)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_8'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to truncate a file which is already open)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kJXchpTpIxdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = HDF5Matrix('train_8.h5', 'train')\n",
        "y_train = HDF5Matrix('train_8.h5', 'train_labels')\n",
        "x_test = HDF5Matrix('test_8.h5', 'test')\n",
        "y_test = HDF5Matrix('test_8.h5', 'test_labels')\n",
        "    \n",
        "model, callback_list = lstm()\n",
        "model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,verbose = 2,validation_data = (x_test, y_test),callbacks=callback_list)\n",
        "    \n",
        "model.save(\"Activity_Recognition.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmMzLsUdlUhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Models - a general overview"
      ]
    },
    {
      "metadata": {
        "id": "dooS0ulglUhW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Many times, we might have to convert one sequence to another. Really?  Where?\n",
        "\n",
        "We do this in machine translation. For this purpose we use models known as sequence to sequence models. (**seq2seq**)\n",
        "\n",
        "If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*3lj8AGqfwEE5KCTJ-dXTvg.png)"
      ]
    },
    {
      "metadata": {
        "id": "SItyOliilUhY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A basic sequence-to-sequence model consists of two recurrent neural networks (RNNs): an encoder that processes the input and a decoder that generates the output. This basic architecture is depicted below.\n",
        "![alt text](https://www.tensorflow.org/images/basic_seq2seq.png)\n",
        "\n",
        "Each box in the picture above represents a cell of the RNN, most commonly a GRU cell or an LSTM cell. Encoder and decoder can share weights or, as is more common, use a different set of parameters."
      ]
    },
    {
      "metadata": {
        "id": "AjptK3QzlUhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the basic model depicted above, every input has to be encoded into a fixed-size state vector, as that is the only thing passed to the decoder. To allow the decoder more direct access to the input, an **attention** mechanism was introduced. We'll look into details of the attention mechanism in the next part."
      ]
    },
    {
      "metadata": {
        "id": "2Mk4NHdFlUhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "\n",
        "Our input sequence is how are you. Each word from the input sequence is associated to a vector \n",
        "w∈Rd (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w0,w1,w2]∈R^{d×3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation e. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$).\n",
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_EAxeXCLl0rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "\n",
        "Now that we have a vector e that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: $e$ as hidden state and a special start of sentence vector $w_{s o s}$ as input. The LSTM computes the next hidden state $h_0 ∈ R h$ . Then, we apply some function $g : R^h ↦ R^V$ so that \n",
        "$s_0 := g ( h_0 ) ∈ R^V$ is a vector of the same size as the vocabulary.\n",
        "\n",
        "\\begin{equation}\n",
        "h_0 = LSTM ( e , w_{s o s} ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_0 = g ( h_0 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_0 = softmax ( s_0 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "i_0 = argmax ( p_0 )$\n",
        "\\end{equation}\n",
        "\n",
        "Then, apply a softmax to $s_ 0$ to normalize it into a vector of probabilities $p_0 ∈ R^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word “comment” has the highest probability (and thus $i_0 = argmax ( p_0 )$ corresponds to the index of “comment”). Get a corresponding vector $w_{i_0} = w_{comment}$ and repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.\n",
        "\n",
        "\\begin{equation}\n",
        "h_1 = LSTM ( h_0 , w_{i_0} ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_1 = g ( h_1 )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_1 = softmax ( s_1 ) \n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "i _1 = argmax ( p_1 )\n",
        "\\end{equation}\n",
        "\n",
        "The decoding stops when the predicted word is a special end of sentence token."
      ]
    },
    {
      "metadata": {
        "id": "dlvMrO4hJKL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg)"
      ]
    },
    {
      "metadata": {
        "id": "6Q9-F7KRQY9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention !!!\n",
        "\n",
        "![alt text](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg)"
      ]
    },
    {
      "metadata": {
        "id": "17fibl9MJieb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq with Attention \n",
        "\n",
        "The previous model has been refined over the past few years and greatly benefited from what is known as attention. Attention is a mechanism that forces the model to learn to focus (= to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is as follows. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM\n",
        "\\begin{equation}\n",
        "h_t = LSTM ( h_{t − 1} , [ w_{i_{t − 1 }}, c_t ] )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "s_t = g ( h_t )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        " p_t = softmax ( s_t )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        " i_t = argmax ( p_t )\n",
        "\\end{equation}\n",
        "\n",
        " The vector c_t is the attention (or context) vector. We compute a new context vector at each decoding step. First, with a function $f ( h_{t − 1} , e_{t ′} ) ↦ α t ′ ∈ R$ , compute a score for each hidden state $e_{t'}$ of the encoder. Then, normalize the sequence of $αt′$ using a softmax and compute c t as the weighted average of the $e_{t ′}$.\n",
        "\n",
        "$α_t ′ = f ( h_{t − 1 }, e_{t ′} ) ∈ R$                 for all $t ′$  \n",
        "\n",
        "$\\vec{\\alpha} = softmax ( α ) $\n",
        "\n",
        "$c_t = n \\sum_{t'=0}^{n}  \\vec{\\alpha}_{t′} e_{t ′}$\n",
        "\n",
        "The choice of the function  $f$ varies"
      ]
    }
  ]
}